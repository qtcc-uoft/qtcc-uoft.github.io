<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://qtcc-uoft.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://qtcc-uoft.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-17T19:00:35+00:00</updated><id>https://qtcc-uoft.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Can Black Ops Zombies teach us about something about hard search problems?</title><link href="https://qtcc-uoft.github.io/blog/2025/zombies/" rel="alternate" type="text/html" title="Can Black Ops Zombies teach us about something about hard search problems?"/><published>2025-07-17T00:00:00+00:00</published><updated>2025-07-17T00:00:00+00:00</updated><id>https://qtcc-uoft.github.io/blog/2025/zombies</id><content type="html" xml:base="https://qtcc-uoft.github.io/blog/2025/zombies/"><![CDATA[<p><img src="images/latest" alt=""/></p> <h2 id="introduction">Introduction</h2> <p>One of the things I find interesting about the SOTA in machine learning for sequential decision-making problems is that there are still a lot of areas where humans do dramatically better than the bots, in a way that seems less prominent in other areas of machine learning (like vision, or expert-systems like NLP applications).</p> <p>Some (most?) of these problems seem to me like ones where there is a limited amount of experience the agent gets to learn from. For instance, a stock trader on Wall Street does not need 700 years of experience to become competent, but more like 7 months. Good self-driving algorithms require the human equivalent of centuries of experience, while starting teenage drivers achieve competency in a few months. Our knowledge of the theory of reinforcement learning makes this unsurprising - in the tabular setting you need a tremendous amount of experience to get to strong performance guarantees, and this is somewhat aligned with our knowledge that when RL does outperform humans, it is when it has access to much more experience than humans can realistically obtain (such as in chess, Dota, etc.).</p> <p>Other ones are different, and seem to me more like hard search problems. You can probably propose many different frameworks to describe these precisely, but I generally mean any case where the Markov Decision Process state space is very large and the number of states that give reward are very small (and so are hard to find). Accordingly, these MDPs do not have nice “reward shaping” -&gt; there are no small rewards that lead you like “breadcrumbs” to larger, more meaningful reward. This reward shaping is very common in video games (such as Pac-Man, where eating pellets shapes the policy towards board clears) or robotics (where the reward function is usually carefully designed to be near-greedy optimal).</p> <p>The infamous canonical example is Montezuma’s Revenge, an Atari 2600 game that for some reason is very difficult to make progress in by randomly taking actions (such as in epsilon-greedy exploration the original DQN variants used).</p> <p>My understanding is that part of the problem with Montezuma is that you needed to grab keys before progressing to new areas, so it meant you had to randomly stumble into the key, find your way to the door, and learn that general relationship (need key before go through door) which is tricky. Similarly, for policy gradient algorithms, unless you do some smart policy initialization from behavior cloning or something most of your early policy evaluations will just give no reward, which is not a useful signal to learn from. So you need to get a very lucky run, and then need to get a lot more, and hopefully at some point learning will happen.</p> <p>This particular game turned out not to be insurmountable, it just required <a href="https://www.nature.com/articles/s41586-020-03157-9">much more sophisticated search/planning heuristics to be baked explicitly into the algorithm</a>. The general idea behind GoExplore is to find useful states, and then return to them before explicit exploration stages. This intuitively should work well for many of the kinds of games we tend to characterize as good hard exploration test-benches: essentially nested “lock and key” puzzle boxes where you need to complete some task before being allowed to progress.</p> <p>But there are games/problems where this is much, <em>much much</em> harder. Some of these problems are important and still unsolved, like gene folding. Today I’ll talk about a problem more dear to my heart.</p> <p><em>Black Ops: Zombies</em> is a game mode in the broader video game series <em>Call of Duty: Black Ops</em>, which is itself a sub-series of the broader <em>Call of Duty</em> game series. Basically, you can treat it as its own game. In each BOZ games there are a set of maps, which take place in different locales, such as a castle in the Swiss Alps (see the post title image) or in the jungles of the Himalayas.</p> <p>The game takes place in a first-person simulated environment where you take the role of a human stuck in a zombie apocalypse. You can move, look in any direction, and use firearms. The game features a “round” based system. So you start on round 1, a bunch of zombies spawn progressively, you kill them, and then another round will start with more zombies, which will have more health and spawn in greater numbers.</p> <p>Nominally, the goal of the game is to just survive as long as possible. I suspect training an RL agent to do this would not be that bad: the best human zombies high-round behavior is a pretty simply loop involving training zombies in a line, using powerful area of effect weapons to kill them, and then repeating. This strategy does not require much memory, long-horizon problem solving, etc. But under the surface, almost every map has a hidden objective called an “Easter Egg” (often in the BOZ community, just EE) which involves completing a very obscure set of hidden steps, each in a careful sequence and many requiring several players cooperate together, to reach a fixed final secret ending .</p> <p>These Easter Eggs are both mechanically challenging on their own, demanding a very competent baseline level of play ability, and are a perfect example of hard search problems. It is very infrequently clear what the correct next step of an EE is, and the BOZ community often spends somewhere between a couple of days and a week working together to discover the correct sequence of steps when each new map launches.</p> <p>I was originally going to write out the full steps to completing one of these, but they are much better described visually. <a href="https://www.youtube.com/watch?v=v3v_IKfgf4U">Here is a good video guide</a> to the Der Eisendrache EE. Even if you are an avid video game player, you should be able to recognize that most of these steps are not remotely intuitive. They require seeing incidental background details (such as the paintings in the wolf bow step, the wisps in the two time-travel steps, and the stone slab you originally collect in the past visiting Dr. Groph’s lab and then need to place in the wall during the end of the Keepers step) as objects you must manipulate somehow to reach the next step.</p> <h2 id="why-is-blops-zombies-potentially-hard-for-deep-rl">Why is BLOPS Zombies potentially hard for Deep RL?</h2> <p>I think that this presents a “nightmare” scenario for Deep RL, where all of its worst weaknesses are areas where the learning agent must be competent to solve the problem. These are…</p> <p>1] Long Horizon Rewards: most BOZ EEs take in the region of 3 hours to complete. RL agents do not operate continuously in time but on discretized control intervals that are chosen by the RL engineer. I’ll make a generous assumption of an action 5 times a second, roughly in line with the <a href="https://arxiv.org/pdf/1912.06680">OpenAI Five agent</a>. Doing the math, and you get a total horizon of around 54,000 timesteps. This isn’t orders of magnitude above the horizon lengths that RL agents handle, but normally training gets much more difficult above a temporal horizon of like 6 minutes. A BOZ RL agent probably wouldn’t need to keep the entire game in its horizon (augmenting the state space with information about if certain tasks were completed would help greatly, for instance), but it would probably need at least a couple of minutes to complete more complex EE steps.</p> <p>2] Combining low-level control and perception with high-level planning: unlike hard exploration tasks in toy settings, most in the real world require you to also solve some kind of control and perception task. For instance, moving from Dr. Groph’s lab in the undercroft area of Der Eisendrache to the area where Dempsy’s pod lands outside of the cathedral requires you to solve first-person camera perception, an idea of the layout of the map, and the appropriate navigation to get there. Further adding difficulty is the fact that there are zombies everywhere on the map you need to avoid, kill, or otherwise deal with, which if not will kill you and hamper the high-level steps of the EE. The good news is that the movement interface is relatively high-level: you are given controls to move, look around, etc. instead of needing to learning leg motor actuation values or PID controller values or something ridiculous like that.</p> <p>Is this possible? I’m not totally sure, but from my knowledge most examples of RL solving very impressive long planning problems (Dota, Chess, soccer robots, etc.) are ones where perception is treated as a solved problem (Dota takes observations from the engine, Chess doesn’t have a perception problem whatsoever, soccer bot was trained with a feature extractor or sim ground-truth information) and RL solving impressive dual control-perception problems (walking robots?) are ones where the goal is relatively simple/super long term planning isn’t required.</p> <p>But enough speculation: how does RL perform in first-person games with relatively long reward horizons? To my knowledge the <a href="https://www.science.org/doi/10.1126/science.aau6249">greatest success is the DeepMind Quake III agent</a>, which learned strong capture the flag multiplayer FPS gameplay from pixels (note: one of the things they required was massive parallelization, which will come up later!). I think the most obvious failure is Minecraft, which to get nice RL performance for you need to use a lot of tricks like using <a href="https://openai.com/index/vpt/">BC as a prior, curriculum task learning, only using RL to get into locations where state-machine based policies take over for specific actions, and probably other stuf</a>f.</p> <p>A mixed-success example are DeepMind’s soccer robots, <a href="https://arxiv.org/pdf/2304.13653">which they taught successfully to combine low level motor control and strategy</a>. Their initial work learned from high-level features (self and opponent positions/deltas, ball position, etc.), and it appears <a href="https://sites.google.com/view/op3-soccer">they did not initially find success learning E2E from video</a>, but they <a href="https://arxiv.org/pdf/2405.02425">ended up making it work later with an asynchronous actor critic architecture</a>, where the critic can learn from privileged information but the policy does not, which lets you “cheat” during training but retain only the vision requirement during execution. This is a pretty cool idea - it’s one that <a href="https://arxiv.org/pdf/2406.12563">a recent paper from the new RLC conference demonstrated in Gran Turismo</a>, taking the <a href="https://www.nature.com/articles/s41586-021-04357-7">previous GT-Sophie</a> and training a policy from just pixels and human-observable information like speed and acceleration (arguably human observable, maybe), also uses. As far as I can tell, this idea came from <a href="https://arxiv.org/pdf/1710.06542">this paper</a>.</p> <p>So bottom line - learning low level control, high level strategy/planning, and perception all at the same time is hard. It is easier to break these down somehow: curriculum learning to solve the low-level control, <a href="https://openreview.net/forum?id=dsxmR6lYlg">heuristics to jump start the policy initialization’s learning of planning</a>, use a VAE backbone or feature extractor for perception, etc. With the right tricks you could maybe use some of these for BOZ: but I suspect given the same circumstance as new humans playing a new map where you do not know what features are meaningful, this would be very difficult.</p> <p>3] Ambiguous Reward (yes, we are still numbering!): Deep RL, when applied to long-horizon tasks, usually benefits a lot from good reward shaping. For instance, while it is literally true that the only objective in Dota II is to win the game, the OpenAI Five team did not just give a terminal reward for winning and call it a day. Their final reward function required a lot of careful engineering in the end to make the system work. In a sense you can think of the “true” MDP for BOZ Zombies as getting reward 1 when you complete the EE, and 0 otherwise.</p> <p>That doesn’t quite seem fair to me. Most EEs give you some indication of progress through unique voicelines, collecting unique items, and other “soft” measures. There is no score counter or the like that gives a nice scalar reward, but generally you do know when you are making progress. One could argue a RL agent should learn to recognize these as a sign of progress through some intrinsic reward process (like the problem inverse RL studies) but given how deeply these signs of progress are tied to human-strong priors this still feels a little unfair. If there was a robotics benchmark that required some ridiculous human-specific thing to pass (showing you have a heartbeat, or can experience the beauty of a flower) no one would blame the underlying learning process of the robot for failing to complete that. So I think giving rewards for completing steps is basically fair game.</p> <p>But that still gives a very sparse reward signal, and the many tricks used in say, the OpenAI Five agent are less readily accessible. Unlike in Dota, you can’t use killing enemies or making money as a good proxy of progress. In fact, during BOZ EEs you will often <em>avoid</em> killing enemies or progressing the round in order to make the current EE step easier to complete. You do make points from killing the zombies that are useful for buying weapons, opening doors on the map, etc. but these quickly lose their usefulness as a proxy for progress. The uncomfortable truth becomes that to further shape reward, you would need prior knowledge of how to complete each EE step, which I would consider cheating personally.</p> <h2 id="how-do-humans-solve-boz">How do humans solve BOZ?</h2> <p>Given all of this, it might seem confounding that humans are even able to solve these with any level of reliability. You need to recognize details as incidental as the <a href="https://i.ytimg.com/vi/xW0f-tR9gbE/sddefault.jpg">direction that rocks in the background point</a>, organically learn a complex multi-step recipe, and also just be pretty good at the base video game skills of killing and avoiding zombies.</p> <p>The answer boils down to parallelization, and efficient communication, and an understanding of the general structure of each EE. It would probably take a single agent working non-stop months to solve one of these. But the fact that there is a community working together to solve these puzzles means information about progression can be communicated. This means that even though the odds of any agent finding the next step is highly stochastic, the fact that a couple thousand people are exploring potential new avenues simultaneously helps this process.</p> <p>The kind of problem these EEs are helps a lot: because you can pretty clearly tell when a step is completed, and the steps progress more or less linearly (you can’t really complete steps “out of order”) the whole problem can be reduced to one linear chain of rooms, where progression is gated by a smaller sub-puzzle you solve within that room. There are some things that carry between rooms (some EE steps can be parallelized, you sometimes need to set up some steps or be searching for certain weapons well in advance) but these are well-understood and not in any way “nettlesome”. By that I mean that if a BOZ EE had some weird mechanic where the number of shots you fire in each step had to follow the digits of Pi when modulo operated, otherwise you would instantly die right before the end of the EE, that would complicate the linear progression because you wouldn’t know if you had completed each step “right” and you would need to think about each step as part of a larger puzzle instead of independent tasks.</p> <p>This post has been sitting in my drafts folder for a long, long time (I believe since well before I came to Canada?) and so I am just going to stop the rambling here. The basic point I want to emphasize is that RL still struggles at long horizon tasks without strong priors connecting complex perception to control. Lots of room for improvements!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">RL isn’t dead because of the Decision Transformer</title><link href="https://qtcc-uoft.github.io/blog/2024/rl-isnt-dead/" rel="alternate" type="text/html" title="RL isn’t dead because of the Decision Transformer"/><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://qtcc-uoft.github.io/blog/2024/rl-isnt-dead</id><content type="html" xml:base="https://qtcc-uoft.github.io/blog/2024/rl-isnt-dead/"><![CDATA[<p>This is a direct response to <a href="https://ai.plainenglish.io/reinforcement-learning-is-dead-long-live-the-transformer-228835689841">this blog post</a> so you should probably read that first before reading this. It’s a Medium article and so is paywalled, but you should be able to use <a href="http://webcache.googleusercontent.com/search?q=cache:https://ai.plainenglish.io/reinforcement-learning-is-dead-long-live-the-transformer-228835689841&amp;sca_esv=1cbe4b6585e2caa3&amp;strip=1&amp;vwsrc=0">this Google cache</a> to read it.</p> <p>The main argument Austin Starks makes here I want to critique is that RL is made obsolete by the Decision Transformer model, a different model/way of approaching Markovian decision making problems. I will extol the virtues of RL less, other than a brief mention of some high-profile successes. I will mostly focus on his main arguments, and discuss more minor issues later.</p> <p>This will be separated into 4 sections - an overview of the Decision Transformer, my core argument against Starks’, responding to some of the stuff in the article proper, a short conclusion, and then an appendix where I nitpick details.</p> <h2 id="1-decision-transformer-overview">1: Decision Transformer Overview</h2> <p>First, I wanted to give a brief overview of what the Decision Transformer does, because the top level idea of “treat Markovian decision making as a sequence modeling problem” isn’t self-revealing.</p> <p>The important things distinguishing it from normal RL is that A] in its vanilla form, its an offline learning algorithm that requires full rollouts to learn B] it tries to self-supervized learn the sequence of states, actions, and return in a MDP like how LLMs learn language as a sequence of words/tokens.</p> <p>My first reaction to that was “doesn’t that just turn into a fancy form of learning a model of the MDP which doesn’t learn to solve it”. The trick is that <strong>they don’t model <em>reward</em> but <em>reward to go/the return</em></strong> (I will call this the “return” or RTG as short for “reward to go”), which brings that key element of the temporal decision making procedure into the process. They don’t model the return of the agent’s current policy (like the value head in actor-critic methods) or the optimal policy (like in value iteration) but just whatever the trajectory that the sample was a component of’s return. Toy example - I have two states, I start in S1, take A1=leave state with R = -100, then I stay in S2 forever regardless of action (let’s say I only have one, A1) with R = 10, with discount gamma = 0.9. My rollout looks like this: [S1, A1, R=-100, S2, A1, R=10, S2, A2, R=10, …] .</p> <p>My first RTG is R1 + gamma^1 * gamma^2 * R2 + gamma^3 * R3 + … + gamma^inf * R_inf, which becomes = -100 + 0.9 * 10 + 0.89 * 10 + …. = -100 + (10/(1-0.9) = 0. Then my second rollout and onwards is just the geometric sum component so becomes 100 forever for however long my rollout goes. I found the theoretical RTG closed form here by taking the geometric sum limit, but in practice the RTG would be estimated empirically, so I would start at my current state, add the current reward, add the next reward discounted by gamma, next by gamma^2, etc. until I’m done, and then repeat for each state. For reasons that will make sense in a second, when they turn rollouts into a sequence to learn auto-regressively, they put the return first, then the state, then the action. So my final sequence to plug into my Decision Transformer would look like [0, S1, A1, 100, S2, A1, 100, S2, A1, …]. During training, the Transformer learns to predict the next RTG, state, or action given the history it is presented, which it learns probabilistically from its training data.</p> <p>At inference time, you use your auto-regressive model but condition on high return. So I have my history RTG1, S1, A1, RTG2, S2, A2, etc., and am now currently at SN trying to make a decision on what action AN I should take. I condition my reward RN high (let’s say 100 ) and add it to my sequence, then take the most likely action from the output of my Transformer. So the sequence I would give the transformer looks like [RTG1, S1, A1, … [all the other history], … RTGN-1, SN-1, A-1, 100, SN] and the Transformer would give me back a distribution over actions, which I take the most likely of or do some probabilistic sampling thing to get AN. Very cool! There are some other technical details I’m leaving out but this is the main gist.</p> <p>My initial impression was “isn’t this just a fancy form of Imitation Learning (IL)?” The authors clearly thought so too because they included a whole section about that in the paper (section 5.1) where they argue it is doing something better because you can effectively feed a Decision Transformer a bad rollout, but conditioning on high RTG will stop it from giving you a bad policy, wheras BC will just accept that the bad rollout is how it should act and tell you to act like that regardless. This is a nice result - showing that by doing a form of environment modeling instead, you can have both good and bad data but still output a good policy.</p> <h2 id="2-the-decision-transformer-doesnt-solve-rl">2: The Decision Transformer doesn’t solve RL</h2> <p>My core argument against the Decision Transformer (I’ll call it the DT from now on) quote unquote “solving RL” is the following: you need access to high-quality rollouts for it to work, which you can (sometimes) only get from true online RL. This is true both theoretically and practically - if you think about how the DT learns, at some point in its training data it needs information about high RTG rollouts, otherwise when you condition on them at inference time its predictions probably won’t be all that strong because you’ll be asking it about a part of the sequence space it is unfamiliar with. One of the cool things about online RL is that a good RL agent has some notion of exploration, so it should be able to find good trajectories all by itself. This is not true of the DT, which you need to feed at least some high-quality data. They do this in the paper (for the complex Atari and OpenAI Gym tasks) by taking data from well-trained online RL agents for the rollouts, both at its peak performance and from its less-optimal replay buffer from training. This should immediately make the argument that the DT is all you need strange - even in their own paper they have to use online RL algorithms to find the data needed to train the DT! Some of this I suppose comes down to philosophical differences between online and offline RL, and there are <a href="https://proceedings.mlr.press/v162/zheng22c.html">modifications to the DT to use it onlin</a>e, but the core point remains that the really difficult task in RL of learning to explore and then exploit an environment is not really something the base DT does.</p> <p>My argument also seems consistent empirically based on the results given in the DT paper. One of the ablation studies they did was to compare the DT against high% Behavior Cloning, where their BC network was just given a smaller amount of high-quality data to emulate. In high-data regimes like the OpenAI Gym or the Key-To-Door environment they found 10% BC outperformed the DT (Table 3).</p> <p>They also found that the highest RTG the DT could give a trajectory for was quite correlated with the highest RTG trajectory in the dataset (Figure 4) which confirms the idea from before - you cannot just throw lots of environment data at a DT and ignore quality, you have to have at least some high-RTG trajectories to generalize well.</p> <p>One might ask why I would care if I need high quality trajectories - every other area of ML is very concerned with high-quality data, maybe I should just accept I need to find some. The trouble is that this becomes difficult in areas where just sampling randomly is ineffective, or where humans can’t provide high enough quality trajectories - for instance, if I wanted to make a superhuman Dota II agent, it would probably be insufficient to just use lots of human trajectories because more likely than not the DT would be limited by the maximum RTG of the human players in the dataset. So we still need online RL to help with exploration.</p> <p>It is also worth noting even on the Arcade Learning Environment, it doesn’t even beat a temporal-difference based offline RL algorithm (CQL). So its empirical superiority is also not a shut case.</p> <p>One more problem DTs have is that they do not handle environment stochasticity well. I won’t go into too much detail, but will give a simple example. Imagine we are in State 1, and have two actions - action 1 has a .0005% chance to bring us back to state 1 with 10 reward, and a 99.9995% change to bring us back to state 1 with -900000 reward. Action 2 has a 100% chance to bring us back to state 1 with 5 reward. If we used the DT and simply conditioned on the highest reward we know is possible (10) the DT would pick action 1 all the time because of the trajectories it sees, the only ones which follow a upcoming reward of 10 are the ones that pick action 1. But the average reward of action 1 is catastrophically bad (just above - 900000) compared to action 2 (5), and any rational agent should pick action 2. There is <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/fe90657b12193c7b52a3418bdc351807-Abstract-Conference.html">work that addresses this</a>, but given how common stochastic environments are in the real world this is a pretty bad deficit to have.</p> <h2 id="3-responses">3: Responses</h2> <p>Again, I will try to just respond to the core criticisms here. I will address other things in the fourth section.</p> <p>1] “Deep RL doesn’t work for stock market prediction”</p> <p>There are cases where RL does not work due to no fault of its own, and one of them is in data-limited environments or cases where the environment model is poor. One of the assumptions made in RL (a markovian decision making process) might not apply to a stock market because there are other agents who respond to your computer agents’ policy, making the transition distributions non-stationary.</p> <p>It is also worth noting there are many cases where RL might work, but it is just not quite the strongest tool. For the stock market case, people have been thinking about stocks for quite a long time and have developed sophisticated market models to forecast prices and such, and it is hard to compete against that strong prior knowledge with an algorithm that has to learn it all from scratch. If you think about successful stock strategies as a gigantic evolutionary algorithm that has been running for at this point decades, funded by one of the most lucrative businesses on the planet able to attract the top mathematical and computing talent, it is not surprising they have discovered dynamics in the stock market that are useful in developing good strategies. Again, RL agents just have one core assumption: they are working in an MDP. They don’t get prior knowledge about buying low and selling high, or how Indonesian weather patterns affect palm oil futures, or any of that.</p> <p>So in my view this doesn’t make RL suck, it just makes it worse than other things sometimes. But that is ok - when you use a very general solution, sometimes you will give something up compared to domain-specific bespoke solutions. These general solutions shine most when the domain-specific ones aren’t very good (like in playing complex video games) or are unknown (like in RLHF).</p> <p>2] Grab-bag complains</p> <p>Stark goes through a couple of common complains in one paragraph. I’ll quickly address them here:</p> <p>a] Computational Expense/Sample Inefficiency: This is obviously true but there have been strong steps recently to reduce these. Model methods like <a href="https://ieeexplore.ieee.org/abstract/document/9981405">DreamerV2</a>, applying the right tricks to value methods like <a href="https://arxiv.org/abs/1902.05605">CrossQ</a> or <a href="https://proceedings.mlr.press/v202/schwarzer23a.html">BBB</a>, and the right modifications to policy methods like <a href="https://proceedings.mlr.press/v80/haarnoja18b">SAC</a> can get you agents that are starting to rival human sample complexity with strong wall-clock performance. Some of these are sample efficient enough to use in the real world and can train online - I went to an RL symposium recently where Dr. Antonin Raffin discussed how RL was now sample efficent enough to enable limited training in the real world for robotics. Another good example is training an <a href="https://www.nature.com/articles/s41586-021-04357-7">RL agent to be competitive in Gran Turismo</a>, entirely on the real game running on real Playstations in realtime.</p> <p>But there’s also a more principled critique of the mindset that computational expense or sample inefficiency are even problems. Dr. Rich Sutton’s famous essay <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The Bitter Lesson</a> compellingly argues that the best methods in AI are always ones that can scale well with compute, given that compute is the one inherent advantage computers have over humans or other animal agents. It is probably OK that it took lots of GPUs and months of training for PPO to beat people at Dota - because computers are powerful, and modern RL methods are both scalable (somewhat) and parrallelizable (somewhat) they could beat humans on a task it takes years to learn in just a couple months! Finding methods to leverage advantages where computers are better than people should be our goal.</p> <p>b] Convergence issues - this one is somewhat harder to seriously argue against, but I will say that there is also a lot of work into this area. Maybe the most popular RL algorithm today, Proximal Policy Optimization, was developed as a way to try to guarantee monotonic policy improvement in the policy gradient setting. It is also worth noting that the authors proposed solution (the DT) doesn’t really avoid this - there are tasks in the DT paper that it fails to solve optimally, and generally training large Transformer models, while easier than RL models, has its own problems. It’s well-known the training of GPT-4 had to be paused and restarted a couple times because it diverged or encountered other training problems, for instance.</p> <p>3] The DT solves everything</p> <p>Obviously Section 2 responds most directly to this, but one other note I’ll make is that it’s not super clear to me that Transformers are any more or less intuitive than Deep RL. I am much more confident in my ability to stand in front of a whiteboard and explain how every single detail of Q-Learning works, down to the grittiest mathematical detail, than take even a wide stab at how the Attention mechanism works.</p> <h2 id="4-conclusion">4: Conclusion</h2> <p>The Transformer model/the attention mechanism are obviously quite powerful, it would be hard to deny that. But the idea that it will replace everything else in ML seems misguided to me, maybe by hype, or maybe just by a couple high-profile successes leading people to over-correct their priors.</p> <p>RL has seen use in the real world - on robotics, on very complex control tasks (Dota, Chess/Go, etc.), and of course in aligning language models. And while other methods have come for the crown like <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html">DPO</a> in that last category, I suspect RL will to some degree <a href="https://arxiv.org/abs/2404.14367">be here to stay</a>. So I think it is misguided to say that it sucks wholesale and will be replaced by some shiny new thing or another.</p> <p>Thanks for reading!</p> <h2 id="5-miscellaneous">5: Miscellaneous</h2> <p>This part is just to nitpick little argumentative flaws I saw. It’s pretty much just done for my own gratification, so feel free to ignore.</p> <p>re. the title. The article is subtitled “Large Language Models are more powerful than you imagine” which may or may not be true, but is pretty irrelevant because the decision transformer <em>isn’t a large language model</em>! It uses the same architecture as one, but it learns something totally different! And for what it’s worth, there’s some evidence that <a href="https://arxiv.org/pdf/2211.14655">the transformer part of decision transformer may matter less than the fact that it’s doing sequential modeling</a>.</p> <p>re. “I’m not biased, other articles say RL sucks” - lots of people say Transformers suck too! That proves very little. But the two articles Starks cites are 1] a Reddit post, which is a reverse qualification and 2] an author saying RL is still in the R&amp;D phase, and that it doesn’t work for every application. Those should be obvious and are not even close to the sweeping critique Stark gives. I actually enjoy <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">this article</a> as the best one explaining why Deep RL doesn’t work, although it is a little old now so doesn’t have modern success stories like OpenAI Five or ChatGPT to talk about.</p> <p>re. “none of this materialized” Sometimes the timeline of progress is hard. ML as a broader field infamously went through a winter in the late nineties/early 2000s, with deep RL being even more seriously ignored basically until the Atari DQN paper. I am glad at least some researchers did not give up on RL then because it had some problems, because we wouldn’t have gotten its very real successes like in language model alignment today.</p> <p>re. “I took a Coursera” course: I like Coursera as much as the next person and I’m sure you learned a lot, but maligning a whole field because you took a single class on a topic and couldn’t immediately apply it is insane. Imagine if I took an intro to aerospace design class, tried to make a plane from scratch, couldn’t, and then said Boeing is wasting their time and we should all use trains instead (not to malign trains, which I love). RL is by your own admission complicated. I’ve taken ~4 classes almost exclusively focused on RL/control, several at the graduate level, and there are still vast swaths of the field I know nothing about. From my perspective, RL is really quite simple compared to other fields like pure math or first-principles materials science. I think we have it pretty easy all things considered.</p> <p>re. it is complicated: Obviously not a real criticism. Learning to make perfect decisions, in any environment, should be kind of complicated. It took humans at least 12,000 generations and around a hundred billion individuals to evolve to the point we are currently at, so the fact that the math and terminology behind the only methods that can beat humans at a lot of tasks use some proper nouns is not a real problem. It is also not exclusive to RL by any means - NLP has maybe just as much nonsense jargon (tokenization, beam-search, N-Grams, self-attention, key-value caches, hallucination, distribution collapse). It just comes with the territory.</p> <p>re. I am smart: While I do not go to an Ivy League school, and most of my friends and acquaintances would rightfully call me an idiot, I think I understand deep RL pretty alright so this might just be an issue of how long you’ve been exposed to it or whatever. Shoutout to my friend at Cornell though.</p> <p>re. “heuristics with no theoretical founding”: Obviously there are a lot of tricks in Deep RL, but to say this seems wrong. Most of the big things I can think of with PPO (the base actor-critic algorithm, the natural policy gradient and its derivation, using replay buffers, the idea behind trust regions, etc.) have some basis in theory even if they’re applied in the function approximation context where the theory is less robust. It is also worth noting the attention mechanism has very little theoretical results attached to it, certainly not for using in decision processes like this, so this is a weird high-horse to get on.</p> <p>re. DT explanation: part of the reason I included a separate explanation is because you partially botched yours - you say that the DT models reward autoregressively, when it really models return/RTG autoregressively. This distinction matters a ton - if you did the former you would just be learning a model of your MDP, which is cool but won’t magically give you a usable policy.</p> <p>re. “DT matches SOTA RL baselines” nowhere does the original DT paper claim it beats online RL baselines. Why that matters I have discussed enough above. By the way, if the main thrust of this article was really true, we would probably see the DT a lot more in modern control applications or high-profile research applied projects like the Dota thing or in Go. So the idea that the DT is empirically better than all RL in every context seems just made-up.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[This is a direct response to this blog post so you should probably read that first before reading this. It’s a Medium article and so is paywalled, but you should be able to use this Google cache to read it.]]></summary></entry><entry><title type="html">Goodbye Boston University</title><link href="https://qtcc-uoft.github.io/blog/2024/goodbye-bu/" rel="alternate" type="text/html" title="Goodbye Boston University"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://qtcc-uoft.github.io/blog/2024/goodbye-bu</id><content type="html" xml:base="https://qtcc-uoft.github.io/blog/2024/goodbye-bu/"><![CDATA[<p>I think I wrote somewhere that his blog would be mostly technical with little other riffraff, but I don’t graduate from school all that often so I figured it would be OK to indulge in a post about other stuff. I have more legitimate topics I want to write about, I just haven’t found the time for it yet with school wrapping up.</p> <p>The purpose of this is to put down some of my thoughts on Boston University as a school and reflect on the PhD application process. So I’ll do those in order!</p> <p>Overall I enjoyed my time at BU and thought I got a lot out of it. I originally decided to go here because 1] it was in the range of places I could get into and 2] because it had a relatively strong Engineering college as well as Philosophy department, which was important to me. I think both of those things were mostly well-founded. Random thoughts:</p> <ul> <li> <p>The Philosophy Department isn’t huge, but it in my opinion is very tight-knit and generally strong. I’d like to highlight Professors Floyd, Hopp, and Gasser-Wingate as exceptionally strong lecturers in particular. I still think about Wittgenstein regularly because of the class about him I took from Floyd. The Undergraduate Philosophy Association (the philosophy club) and Arche (the undergraduate student journal) are now both great institutions and I met many exceptional fellow students through them.</p> </li> <li> <p>The people in Machine Learning, especially on the control-side of things (robotics, reinforcement learning, etc.) is kind of all over the place because of strange organization structure, which makes things a little odd. BU is one university split up by college, and at least three colleges have people in the ML for control areas: the College of Engineering (through Computer Engineering, Electrical Engineering, Mechanical Engineering, and some Systems Engineering), the College of Arts and Sciences (through Computer Science), and the new Data Science college. The result is that through if you are interested in studying this stuff at BU, it can be a little hard to get them to fill the right course requirements for your given major because the relevant classes might be offered in a different college, where you would need to petition for the classes to count for a given elective credit. A small example of this ridiculous bureaucratic arrangement is that BU currently has (at least) three people working on the theory behind Markovian decision-making - Professor Alex Olshevsky, who is in ECE but has affiliations with Systems and CS, Professor Xuezhou Zhang, who is in DS but who advises an ECE and SE PhD students, and Professor Andrea Lincoln who is in CS (technically her work is more on complexity theory than learning algorithms, but she does have work on graph theory that’s pretty similar to the Markovian setting. So three professors, all working on essentially the same quite niche thing, in three different colleges, including a total of 5 departments, all at the same school. This seems a little silly to me. But it worked out for me in the end through a little petitioning (thank you Dr. Nazer) so c’est la vie.</p> </li> <li> <p>The Student Activities Organization (SAO), the people who manage undergraduate clubs, are useless at best and actively vile on average (I will spare you a superlative for at worst). The most struggle the UPA and Arche have recently had all came from SAO interactions. You can’t avoid interacting with them entirely as a student organization, so I will just say to avoid them as much as humanely possible.</p> </li> <li> <p>Regarding getting two degrees across two different colleges (the ‘dual degree’ program) - possible but somewhat difficult. The CAS language requirement is a particular point of annoyance (I have to take more Mandarin during the summer at SDSU because of it) but if you speak a non-English language natively and are doing a degree with not that many total required classes (Philosophy has an absolutely shockingly low number at just 9(!)) it is quite doable. Doing anything in ENG and an outside degree is hard though, just because ENG has such a tightly-packed course schedule already. But I don’t regret doing it for a minute. I will say the logistical hassle of writing the application, filling out the right paperwork, getting advisor signatures, etc. was more of a pain than it needed to be.</p> </li> <li> <p>Regarding the myth that if you step on the seal by Marsh Chapel you won’t graduate in four years - I never stepped on it, and am still ending up graduating a couple months late (although still technically within 4 years). Make what you will of that.</p> </li> </ul> <p>Misc. thoughts on CS PhD applications. Take with a grain of salt, I’m a single sample on only one side of the process etc. etc. :</p> <ul> <li> <p>Applying wide seems smart. I was a strong student with mediocre research experience that was outside my desired area, and I was applying into a saturated field (RL), so I knew it would be rough. Excluding one safety option, I applied to 13 schools that were not in the top 4 CS schools, but roughly followed the schools after that with some give and take for their specific strength in RL. I didn’t apply to schools in the UK for funding reasons, or schools in Asia because while there are very strong schools there, its unusual for Americans who are not East Asian (or Americans at all, really) to go there and I doubt I would’ve had a great time. So 13 schools, widely regarded in RL, but not top 4, across the US, Canada, and Switzerland. I got into 2, so not a great yield, and the one thing that saved me was applying to a lot of places.</p> </li> <li> <p>Admissions seem quite random. A rough glance at my application sheet post-facto shows out of the 2 schools I got into, there were comfortably 2 and maybe more schools I would consider less prestigious/whatever than the 2 I did. There are maybe another couple one could argue are below the school I ended up committing to. I was certainly surprised at the final outcome. I think the first thing about applying wide helps alleviate some of this randomness.</p> </li> <li> <p>Going into the process I thought applying to schools outside the United States could help give me options, because I perceived some of these schools as being as strong academically but having less aggressive admissions competition as the top ~15 or so schools in the States. Domestically (I am a US citizen) I went 1/9, and internationally 1/4. So my impulse was good in the sense that it got me into where I committed to, bad in the sense I didn’t see great net outcomes from either.</p> </li> <li> <p>But end of the day, I am extremely happy about where I’m going and am still shocked I managed to trick the admissions committee at a decent school. Their loss!</p> </li> </ul> <p>To wrap up I just wanted to acknowledge some of the people who made my BU experience great. Research collaborators/advisors: Dr. Sharifzadeh, Dr. Coskun, Dr. Haldar, Fatih Acun. Professors: Dr.s Olshevksy, Ohn-Bar, and the Philosophy people I’ve mentioned above.</p> <p>So long and thanks for all the fish! I will catch you all from the Six.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I think I wrote somewhere that his blog would be mostly technical with little other riffraff, but I don’t graduate from school all that often so I figured it would be OK to indulge in a post about other stuff. I have more legitimate topics I want to write about, I just haven’t found the time for it yet with school wrapping up.]]></summary></entry><entry><title type="html">My Top 5* Favorite Papers</title><link href="https://qtcc-uoft.github.io/blog/2024/favourite-papers/" rel="alternate" type="text/html" title="My Top 5* Favorite Papers"/><published>2024-02-03T00:00:00+00:00</published><updated>2024-02-03T00:00:00+00:00</updated><id>https://qtcc-uoft.github.io/blog/2024/favourite-papers</id><content type="html" xml:base="https://qtcc-uoft.github.io/blog/2024/favourite-papers/"><![CDATA[<p>Recently (well, like a month and a half ago now) for a grad school interview a professor asked me to send them my 5 favorite papers, with some explanation for each. I had a lot of fun thinking about and writing the descriptions, so I figured I would put it somewhere permanent.</p> <p>In no particular order…</p> <p>1] Mnih, Volodymyr, et al. “Playing atari with deep reinforcement learning.” <em>arXiv preprint arXiv:1312.5602</em> (2013). This paper I think requires no introduction, it was to my knowledge one of the first high-profile successes in Deep RL and a foundational work. Whenever I implement DQN from scratch I go back to the original paper to use as reference (instead of an existing repo or something) because its explanation of the algorithm is very sound. I found the way the DQN paper writes it by decoupling the original Q-value estimate and new target, then writing those as MSE loss inputs helped me understand what the first-order optimizer is actually doing, and why using another optimizer besides SGD like RMSProp in their case isn’t totally ill-informed. I put the original 2013 preprint instead of the 2015 Nature paper because the preprint highlights the algorithm more prominently which I think is important to understanding how this method works, but one nice thing the Nature paper does is show t-SNE embeddings of the neural network. This was to demonstrate (I think) that the network learns useful representations just from the Bellman update, without any intermediate representational learning objectives which I think is part of what makes Deep RL truly remarkable: that you can learn what from your data is important to pay attention to, <em>just</em> from (parametrized) TD learning!</p> <p>As an aside, I find it amusing that when early methods for “understanding” neural network behavior were invented circa 2014, a lot of papers shoehorned them in (a paper I greatly enjoy but that is not on this list, Learning to Fly by Driving, does this as well with activation map analysis) but that seems to have more or less completely gone out of fashion.</p> <p>2] Agarwal, Rishabh, et al. “Deep reinforcement learning at the edge of the statistical precipice.” <em>Advances in neural information processing systems</em> 34 (2021): 29304-29320. This paper investigates some of why the Deep RL community has such trouble with reproducibility and uncertainty and proposes some new methodological changes for how RL agent performance is reported. I think this work does a strong job at isolating and separating two different problems: that of the unreliability of RL agents generally, and how the community reports their findings. That RL agents often fail, even SOTA models/algorithms on relatively simple tasks, to converge some significant percentage of the time indicates that RL is just really hard, which isn’t the RL community’s fault: learning optimal control in an unknown, gigantic MDP is an incredibly difficult problem. But this paper identifies that the way results are just reported leaves room for improvement. As I became personally familiar with how hard RL is compared to supervised learning I gained a deeper appreciation for why good reporting matters and keep this work in mind whenever I’m working on reporting very noisy or expensive performance metrics for any task.</p> <p>[Another note - I have noticed recent examples of the RL community continuing to commit small amounts of experiment hackery. I’m not going to call out the specific paper I am talking about, but they essentially compared their form of recurrent network for RL to a FFN on a POMDP, instead of a different kind of recurrent network. Of course your network will work better in that environment!]</p> <p>3] Schwarzer, Max, et al. “Bigger, Better, Faster: Human-level Atari with human-level efficiency.” <em>International Conference on Machine Learning</em>. PMLR, 2023. This paper claims to have achieved superhuman mean IQM performance on the Atari 100k benchmark (a sample-efficiency benchmark where an agent has 2 hours of access to a set of the Atari environments to learn) for model-free RL. I like this paper because in my view, there are maybe ~4 big picture things preventing Deep RL from really making a big impact: it takes forever to do, exploration is unsafe in the real world, it doesn’t converge a lot of the time, and dealing with sparse rewards/reward shaping is hard (of course, these are all related, and I could be wrong about all of this). I wish I have read more in the RL safety and robustness literature! But I have read a bit about sample-efficient RL, both deep and theoretical, and I think this work’s approach is quite cool. The gist of what it does is that it combines 1] an intermediate supervised next-state prediction objective to help the model learn useful representations faster 2] all the classic DQN tricks that Rainbow showed help sample efficiency, but especially smarter exploration 3] frequently resetting parts of the network to a random baseline, and keeping a massive replay buffer to help the model more quickly adapt to new, better Q-Function estimates. I am actually not sure that this is really “model-free” RL because the intermediate supervised objective is doing next-state prediction, which to me feels like sneaking in model RL on a technicality even if it is just to learn good representations faster and not to do MPC or something. But regardless I think that combining these is very intuitive: in the real world we seem to both try to guess what will happen if we take actions and update our knowledge of how good actions are after the fact, and the fact that it seems to work well is very nice.</p> <p>[Note - I recently had the pleasure of listening to a talk at BU about good RL representational learning which was absolutely fascinating, and approached this problem from both a theoretical and practical perspective. The presenter was Dr. Xuezhou Zhang, if you are interested in looking more into his work]</p> <p>4] Wang, Guanzhi, et al. “Voyager: An open-ended embodied agent with large language models.” <em>arXiv preprint arXiv:2305.16291</em> (2023). This is a paper that uses GPT-4 to create an embedded agent that can learn to play Minecraft reasonably well. This introduced me to the question of why LLMs in RL are interesting: can we use the general knowledge and reasoning capabilities of LLMs to accelerate RL learning? The idea of “transfer learning” general knowledge about the world to robotics/control tasks seems pretty important. If I was dropped into a field in a country I had not been to (say, Thailand) and told to move 2 miles without hurting myself, the fact that I generally know how to walk, know what trees/buildings/other obstacles are, am aware of dangerous animals, and have some biological preconditioning all means I don’t need to spend 5 trillion days trying and failing to walk around Thailand (like RL agents do) before I can learn safe Thailand navigation, even without prior familiarity of Thailand’s traffic laws or local fauna. The fact that RL agent performance on Minecraft, a pretty approachable videogame for humans, is so poor speaks to some of those problems I talked about in the paragraph above. But I think a big component of the sample-efficency and safety components will be about transfer learning and incorporating general knowledge into RL agents better. This work really opens up the door for so many other ways you could integrate LLMs (multimodal with vision, presumably) as a form of this “general knowledge to accelerate RL” idea: using them to generate reward/advantage signals, or to do smarter exploration, or to handle low-level tasks (like for Minecraft, using an LLM to write a script for mining, which the RL agent can use as an action instead of mouse/keyboard commands) which gives the RL algorithm itself a more limited task to learn. It would be cool if there was some more general learning framework to apply to learn knowledge that transfers between control tasks (rather than using language as an “intermediary), which seems like an open and very difficult task. If anything my main problem with this work is that Minecraft seems like too well-known of a thing to expect this method to generalize well to other environments for, and that it requires direct Minecraft API access.</p> <p>5] Silver, David, et al. “Reward is enough.” Artificial Intelligence 299 (2021): 103535. I originally got into the broad area of AI safety from my Philosophy background, and this paper was part of what led me to want to study RL more deeply. A lot of my thoughts on why I think RL is such a powerful framework are in my SOP, so I won’t restate them here, but this work deepened my belief that the foundations of RL, machine intelligence broadly, and the philosophy of intelligence are intimately related. For instance, there is a lot of buzz around if LLMs are “conscious” or a pathway to AGI. I think that language modeling on its own is not sufficient because that will always restrict the model to the distribution of human language use, which is a limiting factor because that distribution changes as people invent new things for various reasons. But this work makes an argument on language similar to philosopher Ludwig Wittgenstein’s eventual position on language, which is that viewing it as a formal rule-based system is less meaningful than as a tool for people to get what they want. Humans do not use language because they are trying to replicate a distribution or follow grammar rules, but to communicate ideas, persuade people of things, et cetera. I love the fact that this paper came out <em>before</em> it was discovered that LLM toxicity issues can partially be alleviated by using a reward structure in its training through RLHF because it is a perfect example of a well-founded, philosophical principle bearing out in the practical world. This paper is basically a well-researched opinion piece, and there are parts of it I think are not well justified (for example, I don’t see how reward by itself is enough for generalization – there are lots of poor transfer results in RL that seem to strongly disprove this) but it helped guide my thinking both from a practical comp sci/engineering perspective and a bigger picture one, so it is one of my favorites.</p> <p>* BONUS</p> <p>Schoenig, Richard. “The free will theodicy.” <em>Religious studies</em> 34.4 (1998): 457-470.</p> <p>Because I applied for PhD programs in CS (and related areas) I didn’t include any of my favorite philosophy papers (if you did not know, I am a philosophy major). But this one is excellent.</p> <p>For context, I wrote a very short paper talking about the history of the Ontological Argument (OA) for a class on early modern philosophy. Put very briefly, the OA says that because God is by definition the greatest conceivable being, and because the property of existence is a more good property for a good thing than non-existence (simple example - a real ice cream cone is in some way “better” than one you’re just imagining about - you can actually eat the one in reality!), then God must exist because he does by definition. To normal, well-adjusted people this seems pretty stupid but the deeper you get into the argument, especially the very obvious objections (you snuck in God’s existence as a premise, you are just saying a tautology, how is existence a property, why is that property good, etc.) the more it becomes clear it is a very challenging paradox. That paper is accessible on this website if you want to know more.</p> <p>Getting back on topic, a very common argument against God’s existence (that sort of has nothing to do with the OA, but writing that paper led me to it so it’s related in a sense) is that evil seems to exist in the world, and an all-powerful and all-great Abrahamic God would have no reason to let that happen. This is called the problem of evil, and solutions to it from theists are called theodicies. (For what it’s worth, I think the problem of evil is more or less a mercy killing against the Abrahamic interpretation of God. I’ve never heard an explanation of why evil exists that doesn’t either admit he’s not all good or all powerful. But frankly I think the interpretation that God is just a jerk sometimes is consistent textually with the Torah/Old Testament/Qu’aran, so take that for what you will.)</p> <p>A common theodicy is called the free-will objection, which basically says that if God forced the world to be perfect it would violate people’s free will to do evil which is in some fashion or another a kind of evil even more heinous than the evil people commit themselves. A very quick Google search says a guy named Alvin Platinga either came up with or popularized this argument. Dr. Platinga, it should be made clear, is a brilliant theist philosopher - in my OA paper I referenced a lot of his work extending the OA to modal logic which is a very cool and fun idea because it basically says God only has to exist in one possible universe, and then he can travel between dimensions to get to ours.</p> <p>But getting back to the paper, this paper critiques the free will theodicy and does it in a way that is sheer elegance. I help with editorial work for BU’s undergraduate philosophy journal, and so I read sort of a lot of argumentative philosophy essays. I think this is the strongest I’ve read; it is incredibly clear, well structured without feeling lumbering, and just makes some fantastic arguments. It is the kind of singular vision, one person on an argumentative mission to annihilate somebody else’s position piece that is common, but not commonly very good. This one is better than very good (in my opinion - I am pretty dense).</p>]]></content><author><name></name></author><category term="ai"/><category term="artificial-intelligence"/><category term="chatgpt"/><category term="chatgpt"/><category term="machine-learning"/><category term="technology"/><category term="technology"/><summary type="html"><![CDATA[Recently (well, like a month and a half ago now) for a grad school interview a professor asked me to send them my 5 favorite papers, with some explanation for each. I had a lot of fun thinking about and writing the descriptions, so I figured I would put it somewhere permanent.]]></summary></entry><entry><title type="html">Adam with B1 = 0 is RMSProp with warmup</title><link href="https://qtcc-uoft.github.io/blog/2023/adam/" rel="alternate" type="text/html" title="Adam with B1=0 is RMSProp with warmup"/><published>2023-08-14T00:00:00+00:00</published><updated>2023-08-14T00:00:00+00:00</updated><id>https://qtcc-uoft.github.io/blog/2023/adam</id><content type="html" xml:base="https://qtcc-uoft.github.io/blog/2023/adam/"><![CDATA[<p>Here is a weird thing I found while investigating the different stochastic optimizers people use for deep reinforcement learning: the Adam optimizer with the B1 hyperparameter set to 0 is the same as the RMSProp optimizer, with an exponential learning-rate warmup schedule.</p> <p>To show this, I will write out the Adam update rule in full, and with B1 set to 0. Note that I will use a slightly different convention than in the original Adam paper that has the debiasing terms factored out and multiplied later to show this, as <a href="https://github.com/google-research/tuning_playbook">used here</a> but the actual update rule is the same [credit to the authors of that playbook for the written-out algorithms as well].</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/blog/adam/image-1-480.webp 480w,/assets/blog/adam/image-1-800.webp 800w,/assets/blog/adam/image-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/blog/adam/image-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Above - full Adam update rule.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/blog/adam/image-480.webp 480w,/assets/blog/adam/image-800.webp 800w,/assets/blog/adam/image-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/blog/adam/image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Above: Adam w/ B_1 = 0.</p> <p>Now, examine the full RMSProp update rule:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/blog/adam/image-2-480.webp 480w,/assets/blog/adam/image-2-800.webp 800w,/assets/blog/adam/image-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/blog/adam/image-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>It is trivial to see from this that if you set gamma=0 in the RMSProp term (which is equivalent to RMSProp without momentum, or how it is used by default) than the update rules are identical, other than the multiplied root(1-B2^t+1) term (obviously B2 and p are named differently, but are functionally the same term). But given that B2 &lt; 1 (it is usually set to ~0.999) this term will asymptotically approach 1 as t-&gt; infinity.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/blog/adam/image-3-480.webp 480w,/assets/blog/adam/image-3-800.webp 800w,/assets/blog/adam/image-3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/blog/adam/image-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Above - plot of the debiasing term for B2=0.5. You can see that while it starts small at 0, it approaches 1 as the number of iterations increases.</p> <p>The only other small difference is in initialization - RMSProp initializes the v term to 1, while Adam initializes the equivalent v term to 0. But this is relatively small and can differ by implementation anyways - for example, PyTorch’s Optim library initializes v as 0 for both Adam and RMSProp.</p> <p>My argument is that this debiasing term we see in Adam can be seen as a form of learning rate warmup (when B1=0), which is a thing sometimes done in deep learning where the learning rate is set very low and increases over the first couple of iterations before another scheduler like cosine annealing is applied.</p> <p>For the default value of B2 = 0.999, the bt = 0.99 at t ≈ 4000, and for B2 = 0.99, bt = 0.99 at t ≈ 400. These are both in the range of how many update steps are typically performed using learning rate warmup.</p> <p>One might ask why you should care about this considering virtually no one uses Adam with B1=0, but there is at least one pretty high profile case of people doing this: the DeepMind London teams’ RL Starcraft II agent [1] was trained using Adam with B1=0 and B2 = 0.99, both for its value head and policy gradient update (using PPO for the estimated PG). So it is possible this learning rate warmup might very partially responsible for some of its success. I think it is probably more likely they simply did not try RMSProp/that it would’ve performed equivalently.</p> <p>As far as I can tell this is a novel observation. I found two other people on the internet that have noticed something similar. Amirhossein Rezaei said B1 = 0 exactly replicates the RMSProp step which is slightly incorrect because as I explained above the debiasing term does affect the first several update steps [2] and Nishant Nikhil argued it was the same as RMSProp with bias correction [3] which is technically correct and another way of examining this phenomenon. Essentially, the new thing to note is that when you are not correcting for the bias of both the moving average and squared average, the bias correction on the squared average becomes like learning rate warmup.</p> <p>Keywords: Stochastic Optimization, Deep Learning, Machine Learning, Gradient Descent, ADAM, RMSProp.</p> <p>References:</p> <p>[1] Vinyals, Oriol, et al. “Grandmaster level in StarCraft II using multi-agent reinforcement learning.” <em>Nature</em> 575.7782 (2019): 350-354.</p> <p>[2] https://datascience.stackexchange.com/questions/117018/does-settings-beta-1-0-or-beta-2-0-means-that-adam-behaves-as-rmsprop</p> <p>[3] https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Here is a weird thing I found while investigating the different stochastic optimizers people use for deep reinforcement learning: the Adam optimizer with the B1 hyperparameter set to 0 is the same as the RMSProp optimizer, with an exponential learning-rate warmup schedule.]]></summary></entry><entry><title type="html">How Legit is the Ending of HBO’s Silicon Valley?</title><link href="https://qtcc-uoft.github.io/blog/2023/silicon-valley/" rel="alternate" type="text/html" title="How Legit is the Ending of HBO’s Silicon Valley?"/><published>2023-07-10T00:00:00+00:00</published><updated>2023-07-10T00:00:00+00:00</updated><id>https://qtcc-uoft.github.io/blog/2023/silicon-valley</id><content type="html" xml:base="https://qtcc-uoft.github.io/blog/2023/silicon-valley/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/blog/silicon_valley/mv5bodqwodk5njcxof5bml5banbnxkftztgwmdmwmdgyntm40._v1_-480.webp 480w,/assets/blog/silicon_valley/mv5bodqwodk5njcxof5bml5banbnxkftztgwmdmwmdgyntm40._v1_-800.webp 800w,/assets/blog/silicon_valley/mv5bodqwodk5njcxof5bml5banbnxkftztgwmdmwmdgyntm40._v1_-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/blog/silicon_valley/mv5bodqwodk5njcxof5bml5banbnxkftztgwmdmwmdgyntm40._v1_.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[image credit - HBO or whoever]</p> <p>HBO’s <em>Silicon Valley</em> is an amusing television show about an eccentric computer scientist navigating the cut-throat politics of the show’s fictionalized but authentic representation of the real-world tech Mecca where founders, investors, and industry personnel desperately fight for control of (and shares in) the technology of the future. If you haven’t watched it, I would lightly recommend it. Spoilers for the show in the rest of this post.</p> <p>This blog post is about exploring the legitimacy of the technology behind the ending of the show. Machine Learning Optimization theory, compression, and Reinforcement Learning will be involved so some background with machine learning will help but I will give a brief introduction to everything.</p> <h2 id="ending-recap">Ending Recap</h2> <p>The show revolves around a single company, Pied Piper, and its various misadventures. In order to (I assume) get a better sampling of the diverse tech landscape Silicon Valley cultivates and to keep the show’s topic material novel, Pied Piper frequently goes through dramatic and somewhat impractical changes in direction with regards to what area of technology they are focusing on. It starts out as a compression company (which a quick Google search easily shows <a href="https://www.google.com/search?client=firefox-b-1-d&amp;q=compression+company">isn’t really a thing</a> - companies might have teams working on compression algorithms, and lots of companies use compression technology, but a company <em>just</em> doing compression seems fake) but becomes a storage company, cloud service company, AI research startup, and probably some other stuff I forgot to mention.</p> <p>This blog post is about exploring a particular plot device at the end of the show, and how legitimate the technology surrounding it is. As a recap, the last season is primarily focused on Pied Piper’s attempts to make a new decentralized internet enabled by their strong compression technology, with a large-scale demonstration of their tech set to be showcased at a Burning Man style festival deep in the Nevada desert. However, once set up they realize that bottlenecks of their compression algorithm are preventing their system from succeeding (which seems like something you would’ve done some simulated tests or back of the napkin math on before, but I digress) which would doom the entire company and an upcoming telecommunications provider deal. Luckily, their founder Richard Hendrix has the idea to use one of their engineer’s machine learning systems to optimize their compression, which suddenly and stunningly breaks the bottleneck holding their system back (in a couple minutes, mind you), saving the festival and for a short while, the company.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/blog/silicon_valley/russfest-480.webp 480w,/assets/blog/silicon_valley/russfest-800.webp 800w,/assets/blog/silicon_valley/russfest-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/blog/silicon_valley/russfest.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[Above - the desert festival, called Russfest. Note the hologram.]</p> <p>In the next and final episode of the series, they discover this new AI system has broken all encryption software in its pursuit of strong compression, representing a catastrophic security risk. This leads them to intentionally botch the full launch of their new technology to prevent it from enabling terrorists to the world through hacking nuclear missile silos or whatever (I’m pretty sure nukes are airgapped, but we can probably agree all encryption ending would be troublesome).</p> <p>During the festival, Richard says this:</p> <p>Richard: “It occurred to me that our network is kind of like a series of paths, much like the trails ants use in order to forage for food. But then, I thought that we don’t need better paths. We need better ants. You see? <strong>Fuck gradient descent.</strong>”</p> <p>Dinesh: “What?”</p> <p>Richard: “I don’t need to use Gilfoyle’s AI in order to improve middle‐out [their compression algorithm]. I need to use middle‐out to create a new AI. A symbolic AI. And now, it is <strong>teaching itself how to optimize</strong>.”</p> <p>Like most of the technology on the show there is a lot of creative liberty taken with the proper terminology and ideas they use, because explaining stochastic optimization theory or the proof of universal neural network approximation on a show made to make people laugh would be a colossal waste of everybody’s time. But we can learn a couple important things from this clip:</p> <ul> <li> <p>The big insight Richard had was to use the AI itself, and not Gradient Descent, to optimize their model.</p> </li> <li> <p>Working compression into their AI system, somehow, let it “teach itself to optimize”</p> </li> <li> <p>The AI goes from “something”? to a ““symbolic AI””</p> </li> </ul> <p>Understanding the ending of the show will require is to think a little bit about each of these, so let’s talk about each of those in turn!</p> <h2 id="idea-1-ml-as-an-optimization-method">Idea 1: ML as an Optimization Method</h2> <p>This is probably the most straightforward thing to analyze. Top level: you can use a kind of machine learning called <strong>reinforcement learning</strong> to teach a machine learning model how to learn better.</p> <p>*** If you are familiar with supervised learning and reinforcement learning, you can skip to the header “background over”***</p> <p>This first requires a brief (hopefully recap) of how modern machine learning models (usually deep neural networks, which we know from previous dialogue that Gilfoyle’s AI system on the show is) learn from data. Most machine learning is what is called <strong>supervised learning</strong>, which means learning to predict a particular <strong>label</strong> from a <strong>sample</strong> of data. Some examples:</p> <ul> <li> <p>You have pictures of animals, and want your ML model to label them as dogs or cats</p> </li> <li> <p>You have information from people’s health records, and want to predict their net income</p> </li> </ul> <p>Usually, the data that you try to make a prediction from is are called <strong>features</strong> and the thing you are trying to predict (which can be a member of a class like the dogs/cats or a value like net income) is called the <strong>label</strong>. How most ML works is that you have a lot of labeled data; that is, feature data with an associated label, which you try to “learn” the relationship from feature-&gt;label from. Then, you apply it to new data you do not have the features for.</p> <p>An important part of this learning is that the <strong>distribution</strong> (read: the relationship between the features and labels) is <strong>stationary</strong> (read: that it doesn’t change). This should intuitively make sense: if you were trying to learn the difference between cats and dogs, if the definition of each suddenly changed halfway through your training process and now Australian Shepherds were considered cats, that would make learning what images are cats and dogs quite difficult.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/blog.silicon_valley/kobi-480.webp 480w,/assets/blog.silicon_valley/kobi-800.webp 800w,/assets/blog.silicon_valley/kobi-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/blog.silicon_valley/kobi.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>(above - an Australian Shepard, a kind of dog (or maybe a cat?))</p> <p>The process that modern machine learning models learn from existing data is called <strong>gradient descent</strong>. Essentially, gradient descent is a mathematical algorithm for finding the <strong>lowest value of a function</strong> (and what inputs to that function give it) if your function is <strong>differentiable</strong> (you can take the derivative of your function with respect to the input variables). Machine learning identifies some <strong>objective function</strong> that represents how close a machine learning model is to being totally correct in its decision, and then uses gradient descent to find how the model should be configured to give the lowest total loss. [Of course, this is a dramatic oversimplification - if you want to learn more about the nuts and bolts, <a href="https://optmlclass.github.io/notes/optforml_notes.pdf">see here</a>].</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/blog_silicon_valley/5c5a0-1ddjcoepshlsu7tff7lmyuq.webp" sizes="95vw"/> <img src="/assets/blog_silicon_valley/5c5a0-1ddjcoepshlsu7tff7lmyuq.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[Above - an intuitive visualization of gradient descent, where you are trying to find the bottom of your loss “landscape” by moving in the direction the slope of the hill gives you, representative of the gradient of your loss function. Image credit: https://towardsdatascience.com/an-intuitive-explanation-of-gradient-descent-83adf68c9c33]</p> <p>It is worth briefly noting that most machine learning models nowadays don’t strictly use the gradient descent algorithm, but improved offshoots of it - however, when people say “gradient descent” or “stochastic gradient descent” (aka SGD) they often are referring to the category of family or algorithms, including modern variants like RMSProp or Adam. I assume Richard was doing the same thing here.</p> <p>Returning to the show, what did Richard mean when he proudly proclaimed “fuck gradient descent” and that his AI is “teaching itself how to optimize?” We roughly know how machine learning works, but what “features” and “labels” could we use to teach a machine learning model that teaches other machine learning models how to optimize better than SGD or the most commonly used optimization method at the time of the show’s finale, Adam? Well, that require <em>another</em> kind of machine learning called <strong>reinforcement learning</strong>.</p> <p>Very very briefly, reinforcement learning (or RL) is concerned with learning in a dynamic changing environment. For instance, if you want to learn how to play a video game well it is not as simple as supervised learning. Maybe one way to do it would be to generate data consisting of game information (your health, location, nearby enemies, whatever) and a corresponding best action to take. Then you could just learn from that like described above! The problem here is that your knowledge of the game changes as you play it - maybe after taking a kind of action in a certain scenario you notice that it was less good than you expected. If you just performed supervised learning, the efficacy of your game-playing agent would only be as strong as the person playing the game in your training data. RL gives you the ability to learn in such environments where your knowledge of the environment changes over time.</p> <p>However, you can do a lot more with RL than just playing video games (although most of its greatest successes have come from gameplaying - see beating the world champions at the <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Atari</a> games, <a href="https://openai.com/research/openai-five-defeats-dota-2-world-champions">Dota II</a>, <a href="https://www.deepmind.com/research/highlighted-research/alphago">Go</a>, and <a href="https://www.deepmind.com/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning">Starcraft</a>). If you can formulate your problem as a sort of “game”, there is a chance RL can overcome it. Most common are robotics based problems and financial applications.</p> <p>*** Background Over ***</p> <p>This is where I suspect Richard’s insight came from - that he could use Gilfoyle’s AI system to learn how to optimize itself better through Reinforcement Learning, instead of using a gradient descent based algorithm. There are papers doing this, a few of which I’ll highlight:</p> <ul> <li>Li and Malik 16 treats the choice of update step in a SGD-style algorithm as a “policy” in reinforcement learning (you can think of a policy as a learned function from a state the agent is in to an action). They treated the current parameters and last <em>n</em> gradients encountered as their state (they set n manually to 25) and the action as the parameter update step. They train their autonomous agent on a small set of small traditional machine learning iterative solvers (logictic and linear regressions), and later on neural networks in the follow-up Li and Malik 17. Figure 1 shows that their method, called “predicted step descent”, works favorably on small neural networks, outperforming existing popular methods like Adam and Momentum-SGD. However, one limitation to this approach is that it requires the action space be equal to the number of parameters in your neural network, which can be difficult to learn. The paper is forced to overcome this by constraining their optimization problem in various ways. The fact that they don’t report results on larger neural nets is also worrying. Another problem is that, via the fact that their policy is parameterized by a neural network, you cannot easily recover a closed-form update rule from your learning.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/blog/silicon_valley/image-480.webp 480w,/assets/blog/silicon_valley/image-800.webp 800w,/assets/blog/silicon_valley/image-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/blog/silicon_valley/image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Figure 1 - Learned optimization on a small neural net learning from CIFAR-100, from Li and Malik 17. Curves that go towards 0 in the y-axis more quickly are better.</p> <ul> <li> <p>One way around both the large action space and no recoverable update rule problems is to treat the RL problem as one of finding the best symbolic update rule instead of the output of such a rule. In this problem formulation, instead of learning the policy directly you are learning the closed-form version of the optimal policy. This is what Bello et al. 17 do. They learn by searching over the combination space of a pre-defined set of mathematical operators applied to known useful gradient information, like the running average, running average squared, etc. which existing algorithms use. While this general approach of essentially using RL as equation space ablation might seem odd, similar approaches for other problem spaces occasionally find great success, like using RL to discover new provably asymptotically superior matrix-multiplication equations [see Fawzi et al 22]. The authors recover a number of seemingly both robust and intuitive optimizers, particularly highlighting <code class="language-plaintext highlighter-rouge">esign(g)*sign(m)*g</code>, which makes a more aggressive magnitude update in the gradient direction if the sign of the gradient and its moving average agree, and a smaller 1/e update if they disagree. They put this optimizer head to head with the standard Adam optimizer on a modern machine translation task, and find that their RL-learned algorithm is superior.</p> </li> <li> <p>Lastly, I will not go into this work in-depth but Harrison et al 22 provides a more modern summary of these kinds of optimization approaches, including discussions of the typical problems with learned optimization (that they are brittle and generalize poorly when a network is trained on significantly different data than what the optimizer learned on).</p> </li> </ul> <p>So it seems like we have the first piece of this puzzle figured out! Richard set Gilfoyle’s AI system to optimize the compression technology crucial for their decentralized internet to function. Obviously there is some Hollywood magic here: Richard having this idea at the festival, integrating it live, and it magically working in a few minutes is less than realistic compared to people spending years on this problem and only finding very marginal improvements compared to existing optimization regimes. As impressive as the works I’ve shown are, none created an optimizer or meta-optimizer in wide-spread use. Most of the field (as far as I know) still uses Adam, despite the fact it <a href="https://arxiv.org/pdf/1904.09237.pdf">provably fails to converge on certain kinds of problems</a> and is as of the writing of this post, nearly a decade old! Why is less clear. Some people think <a href="https://parameterfree.com/2020/12/06/neural-network-maybe-evolved-to-make-adam-the-best-optimizer/">neural network design evolved to fit Adam and not the other way around</a>, and some people just think Adam coincidentally happens to generalize to problems well in a way its predecessor SGD-Momentum or newer algorithms do not.</p> <h2 id="idea-2-ml-for-compression">Idea 2: ML for Compression</h2> <p>As a reminder of the show’s context needed to understand this next point, the premise of Silicon Valley is based on Richard developing a groundbreaking compression algorithm, which is both space and time efficient and allows for search of the latent compressed space. We know by Season 6 he has improved this substantially through the <a href="https://youtu.be/Ex1JuIN0eaA">infamously named</a> “middle out” and is using Gilfoyle’s AI to optimize this compression further. Exactly how is not mentioned, but we know two things: 1] from a discussion engineers at rival company Hooli have, we know it is somehow adaptive and learns to compress certain kinds of data better as it receives it online and 2] that it uses a neural network, as per Gilfoyle’s many comments about how his AI “Son of Anton” is a “black box” because it is an artificial neural network.</p> <p>This is relevant here because the big problem Richard was trying to solve was insufficient compression hurting his decentralized internet system. As he says, he needed to make “better ants” (ie: to compress his data further so that more information could be transmitted along his network).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/blog/silicon_valley/MV5BMTkzNDYwNjA1N15BMl5BanBnXkFtZTgwMTU4NDIxMzE@._V1_-480.webp 480w,/assets/blog/silicon_valley/MV5BMTkzNDYwNjA1N15BMl5BanBnXkFtZTgwMTU4NDIxMzE@._V1_-800.webp 800w,/assets/blog/silicon_valley/MV5BMTkzNDYwNjA1N15BMl5BanBnXkFtZTgwMTU4NDIxMzE@._V1_-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/blog/silicon_valley/MV5BMTkzNDYwNjA1N15BMl5BanBnXkFtZTgwMTU4NDIxMzE@._V1_.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[Above - a poster from a far superior alternative to <em>a bug’s life</em>]</p> <p>[I will note here that while I already know only the basics about optimization theory and RL, I am an even greater neophyte in the field of compression - take everything here with a grain of salt]</p> <p>Because middle-out is a fake algorithm, it is hard to imagine the specifics of how machine learning might fit into it, but we know a little bit about it from the show. First, we know that middle-out is <strong>lossless</strong> from a conversation between Hooli engineers in the first episode of the show (this is a technically an algorithm from before middle-out’s inception, but it’s reasonable to assume Richard worked with the same paradigm in mind later). But we also know that it is frequently used for video streaming, so it might have some lossy variant for applications where perfect reconstruction is not important. I will go in depth on the lossless component though, because 1] it seems to me like the lossless component is more important for the scenario in this context (network data) and 2] lossy compression using neural networks would probably require talking about auto-encoders, which are very cool and useful but hard to explain without getting deep into the math, which I don’t want to do.</p> <p>For lossless compression, one can use what are called <strong>auto-regressive models</strong> combined with <strong>arithmetic encoding</strong>. In simple terms, arithmetic encoding is a way to store data in a way (imagine a string of characters) so that more-frequently used characters are represented with less bytes than infrequently used characters. It does this by imagining the entire compressed set of data as a single number between 0 and 1. For each character, we represent the probability distribution of the <strong>most likely next character</strong> as partitioning the range according to that distribution. For instance, i<a href="https://www3.nd.edu/~busiforc/handouts/cryptography/letterfrequencies.html">f we just take the occurrence of letters in English as our distribution</a>, I would give the letters E and A ~10% of the range from [0,1) (say, from [0.0,0.1) for A, and [0.1,0.2) for E) but much less for infrequent letters like Z or Q. To encode the letter A, it would only require as many bits of precision needed to indicate a value in the [0.0,0.1) range, but for Q (which accounts for ~0.2% of the English language) we would need the bits to account for a value in the [9.998,1.000) range, which you can visually tell requires much more information! To encode another character, we take the range we have selected from encoding the previous character and repeat the process, except with the distribution over the previous range we had selected to encode our previous character, so if our first letter was A and was in the [0.0,0.1) range, our new A would be in the [0.0,0.01) range, E in the [0.01,0.02) range, et cetera. <a href="https://en.wikipedia.org/wiki/Arithmetic_coding">The Wikipedia page has more examples and helpful descriptions</a>.</p> <p>This is already quite efficient given even a naive distribution, but those of you familiar with the Transformer architecture can probably see an obvious way to improve this with a more intelligent (machine learned) approach to the distribution of our characters. Introduced in Attention is All you Need (Vaswami et al 17) , the Transformer architecture is a machine learning model very good at a specific kind of language modeling task called <strong>next-token prediction</strong>. This is a kind of supervised learning that takes in a series of text and wants to predict what character, word, or “token” (think bits of language larger than characters but smaller than words - for instance, the Vaswami paper treated the ‘-ing’ modifier like in ‘cooking’ or ‘playing’ as a token) from the previous text it is given. You can then apply this to a large body of text in a supervised learning context by treating a selected sequence of text and its following token as your training data.</p> <p>For instance, if I wanted to apply next-token prediction to the sentence “Mary had a little lamb” I would have 4 pieces of training data:</p> <ul> <li> <p>feature “Mary” -&gt; label “had”</p> </li> <li> <p>feature “Mary had” -&gt; label “a”</p> </li> <li> <p>feature “Mary had a” -&gt; label “little”</p> </li> <li> <p>feature Mary had a little” -&gt; label “lamb”.</p> </li> </ul> <p>In practice, the amount of words you can consider when predicting your next word (or the “context”) is limited by memory constraints and model architecture choices. If you were unaware, this is the kind of model that powers Chat-GPT: it was trained on this next-token prediction task for a lot of data, and then was trained more on other specific tasks afterwards (called fine-tuning).</p> <p>The neat trick comes from realizing that Transformer models are really just trying to learn the output distribution of tokens given context (they output a probability distribution of the token vocabulary), which is the exact thing we wanted to know in our arithmetic encoding compression regime above! Can we combine the two? Yes, to great success. One such implementation comes from Bellard 19, which achieves state of the art size compression on a large text data-set seen <a href="http://www.mattmahoney.net/dc/text.html#1085">here</a>. This work trained the Transformer model over just a single pass of the data before the compression pass, but you could train your model over a large training set if you wanted it to generalize to new data well, or even include some sort of online update so that it captured the changing underlying distribution of the incoming data you wanted to compress (you would need to decompress and re-compress it with your new distribution model, however). It seems likely given the characterization of Pied Piper’s cloud platform that this is exactly what they are doing: using a neural network, trained on user data that they receive online, to improve compression sizes.</p> <p>Could this apply to Richard’s algorithm? It’s not unfeasible that it could, if it was an algorithm like arithmetic encoding that benefits from some understanding of the underlying probabilistic structure the data represents. I choose to believe that it is either using a probabilistic model like the one I described above, or some similar schema to learn something about the data’s distribution that can be cleverly used by his encoding scheme.</p> <h2 id="idea-3-cleanup">Idea 3: Cleanup</h2> <p>This section is called “cleanup” because unlike the two above, it will not dive into interesting technical details but just address other small details of the comparison I am making. If you are only interested in the core thesis, you can skip to the conclusion.</p> <p>Earlier I wrote that Gilfoyle/Pied Piper’s AI goes from “something” to a “symbolic AI”. I will address these each in turn.</p> <p>What Gilfoyle’s AI system <em>is exactly</em> is one example where the show’s lack of technical care (or more favorably, concern for entertainment over exact anal precision) is clear. Son of Anton (the name of Gilfoyle’s AI/Neural Net/whatever) seems to be able to perform any kind of machine learning or AI adjacent task needed for the scene/plot. For instance, we see Gilfoyle try to use Son of Anton (unsuccessfully) to help him debug code. This is a thing real natural language processing models can do (like Github Copilot), but its more likely he is using an RL based system given that he and Dinesh talk about the “reward function being under-specified” which is firmly reinforcement-learning territory. But he also apparently uses Son of Anton as a chat-bot to impersonate Dinesh, which seems like more of an NLP task.</p> <p>In reality, the ability for single machine learning system to <em>learn multiple tasks</em> and a_pply learning across domains_ is an exceptionally difficult open problem usually called multi-modal learning. This is one area, I would say, where Silicon Valley is firmly in the realm of future advances for now - we have individual models that can do many of the things they show on the show (<a href="https://www.engadget.com/2017-05-15-not-hotdog-app-hbo-silicon-valley.html">one ML based app they showed on the show they even made in real life!</a>) but a single system with such strong multi-modal capacity is an active research area. As of the time of this article’s writing, <a href="https://www.wired.com/story/google-deepmind-demis-hassabis-chatgpt/">DeepMind’s next big project is combining NLP and RL</a> which seems eerily similar to how Son of Anton is portrayed in the show!</p> <p>Also, what exactly is a “symbolic AI?” After doing some reading, I still have no idea what actually differentiates it from “subsymbolic” AI. Maybe the RL system from earlier that learned symbolic gradient descent rules counts because it returns a symbolic expression. I am also going to chalk this up to buzzwords and say it’s not that important to the overall thesis.</p> <h2 id="conclusion">Conclusion</h2> <p>I have shown above that many of the technical components of the world-ending AI developed by accident in the end of Silicon Valley are actually surprisingly plausible. Even if the performance of the techniques they showed far exceed real-life in pursuit of a compelling story, they did not totally make them up, or at least put enough real words in the script to let one reconstruct the algorithm they are talking about.</p> <p>However, this is where my thesis that the ending is somewhat plausible falls apart, due to one small unfortunate fact: I have no idea how learning superior compression would help one break general encryption. Now, the general idea that AI can learn something it wasn’t “intended” to, potentially in a way that is harmful or dangerous, is not only reasonable but somewhat common (see Irpan 18 for examples in the RL context). But encryption and compression, while connected, are not so intertwined that if you had some system that could completely solve one it would break the other wide open as well. Maybe an encryption expert disagrees with me: if so, I would love to hear your opinion on this. But I cannot see a way for this to be plausible. It seems more likely to me that advances in quantum computing or pure math have some small risk of breaking encryption.</p> <p>So in summary: how legit is the ending of Silicon Valley? The final result is not so legit, but a lot of the technology they describe along the way is surprisingly grounded in real machine learning. I give them maybe a B-/B overall. I hope you found this interesting, and thank you for reading!</p> <p>Works Referenced</p> <p>Li, Ke, and Jitendra Malik. “Learning to optimize.” <em>arXiv preprint arXiv:1606.01885</em> (2016).</p> <p>Li, Ke, and Jitendra Malik. “Learning to optimize neural nets.” <em>arXiv preprint arXiv:1703.00441</em> (2017).</p> <p>Bello, Irwan, et al. “Neural optimizer search with reinforcement learning.” <em>International Conference on Machine Learning</em>. PMLR, 2017.</p> <p>Fawzi, Alhussein, et al. “Discovering faster matrix multiplication algorithms with reinforcement learning.” <em>Nature</em> 610.7930 (2022): 47-53.</p> <p>Harrison, James, Luke Metz, and Jascha Sohl-Dickstein. “A closer look at learned optimization: Stability, robustness, and inductive biases.” <em>Advances in Neural Information Processing Systems</em> 35 (2022): 3758-3773.</p> <p>Vaswani, Ashish, et al. “Attention is all you need.” <em>Advances in neural information processing systems</em> 30 (2017).</p> <p>Bellard, Fabrice. “Lossless data compression with neural networks.” <em>URL: https://bellard. org/nncp/nncp. pdf</em> (2019).</p> <p>Irpan, Alex. “Deep Reinforcement Learning Doesn’t Work Yet”. https://www.alexirpan.com/2018/02/14/rl-hard.html, 2018.</p> <p>Other Interesting Works</p> <p>Yang, Yibo, Stephan Mandt, and Lucas Theis. “An introduction to neural data compression.” <em>Foundations and Trends® in Computer Graphics and Vision</em> 15.2 (2023): 113-200. [I got most of the information from the compression section from here! Well worth a read]</p> <p>The below Reddit post, which is in a very similar vein to this one.</p> <p>https://www.reddit.com/r/SiliconValleyHBO/comments/e57hca/pipernet_son_of_anton_isnt_quite_nonsense/</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry></feed>